<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<?xml-stylesheet href="./_c74_ref.xsl" type="text/xsl"?>

<c74object name="ml_tf_inferencer" module="ml_tf_inferencer">

    <digest>
	Runs Tensorflow neural network in MAX for inferencing.
    </digest>

    <description>
    <ul>
	"ml_tf_inferencer" is a MAX/MSP extension object that can load and run TensorFlow/Keras neural network by running the Python server. The python server will run Tensorflow library that is supporting GPU use. Thus, inference will be acquisitively fast use the GPU if available (and all drivers properly installed) on the machine. The object takes the network's name, input matrix sizes and input data as arguments. For the object to function you need to have Python installed on your machine, with some libraries as: Tensorflow, pythonosc, numpy. Please see README file for detailed instunction!
	</ul>
    </description>

    <!--METADATA-->
    <metadatalist>
	<metadata name="author">Tornike Karchkhadze</metadata>
	<metadata name="tag">ml_tf_inferencer</metadata>
    </metadatalist>

    <!--INLETS-->
    <inletlist>
    </inletlist>

    <!--OUTLETS-->
    <outletlist>
    </outletlist>

    <!--ARGUMENTS-->
    <objarglist>
		<objarg name="filename" optional="1" type="character">
			<digest>
			Tensorflow/Keras network.
		    </digest>
		    <description>
			Name of the Tensorflow/Keras neural network saved in h5 format.
		    </description>
		</objarg>
			<objarg name="input_size" optional="1" type="list">
			<digest>
			Input matrix shape given by list. 
		    </digest>
		    <description>
			Input matrix shape given by list. 
		    </description>
		</objarg>
    </objarglist>

    <!--ATTRIBUTES-->
    <attributelist>

    	<attribute name="gpu" get="1" set="1" type="int" size="1">
		    <digest>
			Use GPU of the computer.
		    </digest>
		    <description>
			This attribute gives posibility to deactivate or limit GPU usage by puthon server. @gpu -1 means GPU use is not limited and python sevrer will use whatever possible, 0 meand GPU off, any integer number (for example 2048) means amoung of MB of GPU that will be used.
		    </description>
		</attribute>
    	<attribute name="verb" get="1" set="1" type="int" size="1">
		    <digest>
			Verbose data processing and timing.
		    </digest>
		    <description>
			This attribute gives control on log data. When @verbose is 1 object will print execution times and details of data flow. It is usefull to see how much your prediction takes.
		    </description>
		</attribute>

    </attributelist>

    <!--MESSAGES-->
    <methodlist>
		<method name="data">
			<digest>
			Followed by float is data input.
		    </digest>

		    <description>
			The message "data" followed by floating point number or list of float numbers will be considered as data input and will be put in buffer for sending to python server.
		    </description>
		</method>

		<method name="end">
			<digest>
			Triggers data sending to server.
		    </digest>
		    <description>
			The message "end" means the end of data input and the start of prediction procedure. It triggers data sending to server, that is followed by prediction procedure and receiving data back form the server.
		    </description>
		</method>


		<method name="read">
			<digest>
			Load Tensorflow network.
		    </digest>
		    <description>
			The message "read" followed by tensorflow network name that is located in the same directory as MAX patch or in other known directory for MAX (directories in file prefferences) loads network .h5 file into python server. A message "read" without filename opens dialog box to chose file.
		    </description>
		</method>

		<method name="input_size">
			<digest>
			Input matrix shape.
		    </digest>
		    <description>
			Input data matrix shape, asssuming that it will not have more dimentions than 10.
		    </description>
		</method>

		<method name="server">
			<digest>
			Python server on/off.
		    </digest>
		    <description>
			Sending 1 or 0 with message "server" starts and stops python server.
		    </description>
		</method>

		<method name="gpu_change">
			<digest>
			Control GPU usage.
		    </digest>
		    <description>
			Sending intreger numneber with message "gpu_change" sets limit on GPU usage of to Tensorflow that runs in the python sevrer. -1 means no limit and server will use whatever is possible (if the computer has CUDA supported GPU card and CUDA drivers are installed), 0 means no GPU usage, any integer greater then 0 will limit GPU memory use (recomended amounts are powers of 2 = 1024, 2048, 4096...). Changing setiing of GPU restarts server. 
		    </description>
		</method>

		<method name="verbose">
			<digest>
			Verbose running log data.
		    </digest>
		    <description>
			Sending 0 or 1 with message "verbose" sets verbose flag of the object. 0 means no logs will be showed. 1 means, the object will send log data to MAX console. It will show status of the server, network file it is reading, input data shape expected, gpu status, the port numbers it is listening and sending to. In time of prediction it will print execution time data in miliseconds.
		    </description>
		</method>

		<method name="clear">
			<digest>
			Cleaning object from all settings.
		    </digest>
		    <description>
			Sending "clear" message restarst server (if running) and cleans all the settings, that inculdes: the network its running, input data shape, gpu setting (sets to -1), verbose setting (sets to 0). Use it to build process form the scratch.
		    </description>
		</method>

    </methodlist>


</c74object>
